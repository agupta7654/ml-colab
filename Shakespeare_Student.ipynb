{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agupta7654/ml-colab/blob/main/Shakespeare_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c217550-ac39-4cd8-9772-8775df1f9345",
      "metadata": {
        "id": "3c217550-ac39-4cd8-9772-8775df1f9345"
      },
      "source": [
        "# Generating Shakespeare"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f41700ae-dae2-4280-9207-2501b2857160",
      "metadata": {
        "id": "f41700ae-dae2-4280-9207-2501b2857160"
      },
      "source": [
        "You will create a small RNN network to learn how to write Shakespeare text letter by letter. Unfortunately these types of model take a very long time to train (hours) on a decent GPU so your results today in class won't be optimal. They may still impress you."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6526346-1657-45f4-9861-52937754ed4d",
      "metadata": {
        "id": "f6526346-1657-45f4-9861-52937754ed4d"
      },
      "source": [
        "First load the dataset from the intenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b8d2c0-e08e-4250-bea5-700ebb267a81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73b8d2c0-e08e-4250-bea5-700ebb267a81",
        "outputId": "415ad9c6-145a-4256-b8c9-49f9b8b328f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded Shakespeare text. Length: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# Download the file\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Print some info\n",
        "print(\"Downloaded Shakespeare text. Length:\", len(text), \"characters\")\n",
        "print(text[:100])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0db65b4-5766-4920-80b5-fd6bbe4365a0",
      "metadata": {
        "id": "e0db65b4-5766-4920-80b5-fd6bbe4365a0"
      },
      "source": [
        "You need to transform this into an array of integers instead of characters. Use the sklearn LabelEncoder. You should find 64 distinct characters. To be sure, print out all the encoded integers and the character they correspond to. *If you want* you can lowercase all the letters first. This may speed up training some."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46b6a32-a662-4f2c-834f-e5d109784b7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c46b6a32-a662-4f2c-834f-e5d109784b7e",
        "outputId": "08a504f6-4cd7-4cb5-f4d4-48bbad031115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ": 0\n",
            " : 1\n",
            "!: 2\n",
            "$: 3\n",
            "&: 4\n",
            "': 5\n",
            ",: 6\n",
            "-: 7\n",
            ".: 8\n",
            "3: 9\n",
            ":: 10\n",
            ";: 11\n",
            "?: 12\n",
            "a: 13\n",
            "b: 14\n",
            "c: 15\n",
            "d: 16\n",
            "e: 17\n",
            "f: 18\n",
            "g: 19\n",
            "h: 20\n",
            "i: 21\n",
            "j: 22\n",
            "k: 23\n",
            "l: 24\n",
            "m: 25\n",
            "n: 26\n",
            "o: 27\n",
            "p: 28\n",
            "q: 29\n",
            "r: 30\n",
            "s: 31\n",
            "t: 32\n",
            "u: 33\n",
            "v: 34\n",
            "w: 35\n",
            "x: 36\n",
            "y: 37\n",
            "z: 38\n"
          ]
        }
      ],
      "source": [
        "# your code\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "text = text.lower()\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "data = encoder.fit_transform(list(text))\n",
        "\n",
        "distinct_char = len(Counter(data))\n",
        "\n",
        "freq = Counter(data)\n",
        "\n",
        "# print(\"Num distinct characters:\", len(Counter(data)))\n",
        "\n",
        "# print(np.(data))\n",
        "\n",
        "for original, encoded in zip(encoder.classes_, range(len(encoder.classes_))):\n",
        "    print(f\"{original}: {encoded}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a088c6f3-0c55-43ab-a4ca-c87bdcb8c96d",
      "metadata": {
        "id": "a088c6f3-0c55-43ab-a4ca-c87bdcb8c96d"
      },
      "source": [
        "Now as you did last class, convert this single array into X,y pairs, where each row of X is a string of characters and each y is the next character. For example\n",
        "\n",
        "'to be or not to b', 'e'\n",
        "'what light throug', 'h'\n",
        "\n",
        "You can choose how long you want the string of X chars to be (64,128,256 -- something in this range is reasonable. Smaller is faster to train. Longer makes a smarter model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f39f5dc8-de9d-4920-b72c-d5b311acbe90",
      "metadata": {
        "id": "f39f5dc8-de9d-4920-b72c-d5b311acbe90"
      },
      "outputs": [],
      "source": [
        "# your code\n",
        "import numpy as np\n",
        "X = []\n",
        "y = []\n",
        "size = 100\n",
        "\n",
        "for i in range(size, len(data)-1):\n",
        "  X.append(data[i-size:i])\n",
        "  y.append(data[i+1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22b6612a-d8c8-4c26-8bfa-881402cf1c1b",
      "metadata": {
        "id": "22b6612a-d8c8-4c26-8bfa-881402cf1c1b"
      },
      "source": [
        "Create a train/test set by choosing the first say 80% of the data for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad48f791-c5e3-4eb1-bec8-8c9563cfe64e",
      "metadata": {
        "id": "ad48f791-c5e3-4eb1-bec8-8c9563cfe64e"
      },
      "outputs": [],
      "source": [
        "# your code\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "X_train = np.array(X[:(len(X)*4)//5])\n",
        "y_train = np.array(y[:(len(y)*4)//5])\n",
        "X_test = np.array(X[((len(X)*4)//5+1):])\n",
        "y_test = np.array(y[((len(y)*4)//5+1):])\n",
        "\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "X_test = np.array(X_test, dtype=np.float32)\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=distinct_char)\n",
        "y_test_one_hot = to_categorical(y_test, num_classes=distinct_char)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06e32270-a155-46c1-9fed-6f70a0f4262f",
      "metadata": {
        "id": "06e32270-a155-46c1-9fed-6f70a0f4262f"
      },
      "source": [
        "Input to an RNN needs to be a 3D tensor. You will probably need to reshape your data.\n",
        "\n",
        "```python\n",
        "# Reshape the input data for LSTM (samples, timesteps, features)\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "```\n",
        "\n",
        "For example if X_train.shape is (1000,100,1) then you have 1000 phrases each of length 100. The '1' wraps this in a 3D tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa0c40cf-c090-41f5-a5e0-88d059f766cd",
      "metadata": {
        "id": "fa0c40cf-c090-41f5-a5e0-88d059f766cd"
      },
      "outputs": [],
      "source": [
        "# Reshape the input data for LSTM (samples, timesteps, features)\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782aee1a-eb6b-482a-871d-a36ded7eef4e",
      "metadata": {
        "id": "782aee1a-eb6b-482a-871d-a36ded7eef4e"
      },
      "source": [
        "Define your RNN. Use one layer of RNN -- you can choose SimpleRNN, LSTM, or GRU with similar semantics. Here is an outline\n",
        "\n",
        "```python\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Input([None,1])\n",
        "model.add(GRU(128)) # 128 hidden units in one GRU layer\n",
        "model.add(Dense(alphabet_size, activation='softmax'))\n",
        "```\n",
        "\n",
        "The input is a sequence of *any length* (hence the `None`), but only 1D (characters). The output is a 1-hot encoded vectors over each character. Train this using cross entropy and adam optimizer. You can pick any batch size (larger is faster, consult the GPU memory usage). Don't expect super high accuracy, train only for a few epochs (10 or less, maybe much less! Start with 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce09d31-8331-4f2a-9070-eeea05c652b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dce09d31-8331-4f2a-9070-eeea05c652b8",
        "outputId": "202ee787-79cd-481e-b677-4ab81ce457e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m393/393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 51ms/step - accuracy: 0.1747 - loss: 3.0191 - val_accuracy: 0.2016 - val_loss: 2.8599\n",
            "Epoch 2/5\n",
            "\u001b[1m393/393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 49ms/step - accuracy: 0.2018 - loss: 2.8449 - val_accuracy: 0.2051 - val_loss: 2.8142\n",
            "Epoch 3/5\n",
            "\u001b[1m393/393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 49ms/step - accuracy: 0.2128 - loss: 2.7857 - val_accuracy: 0.2186 - val_loss: 2.7726\n",
            "Epoch 4/5\n",
            "\u001b[1m393/393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 49ms/step - accuracy: 0.2300 - loss: 2.7319 - val_accuracy: 0.2251 - val_loss: 2.7422\n",
            "Epoch 5/5\n",
            "\u001b[1m393/393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 51ms/step - accuracy: 0.2417 - loss: 2.6847 - val_accuracy: 0.2298 - val_loss: 2.7219\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, Dense, GRU\n",
        "\n",
        "model = Sequential([\n",
        "  GRU(128, return_sequences=False),\n",
        "  Dense(distinct_char, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train_one_hot, batch_size=2048, epochs=5, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.predict(X_train[0]))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsW0pIn20G7f",
        "outputId": "8597247b-91b0-4fd0-ff3a-44e797498368"
      },
      "id": "gsW0pIn20G7f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "[[0.02921009 0.01414633 0.00218443 ... 0.00464458 0.02560094 0.0054914 ]\n",
            " [0.02592544 0.01235034 0.00187051 ... 0.00453177 0.02597136 0.00478361]\n",
            " [0.02076109 0.00959676 0.00149703 ... 0.00434594 0.02778262 0.00371115]\n",
            " ...\n",
            " [0.01895906 0.00852731 0.00139156 ... 0.00425165 0.02917691 0.00332667]\n",
            " [0.02197204 0.01025897 0.00157351 ... 0.00439531 0.027139   0.00396281]\n",
            " [0.0198519  0.00907494 0.00144293 ... 0.00430254 0.02840696 0.00351932]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yaPRNwc2bmnh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaPRNwc2bmnh",
        "outputId": "a3cff0da-bb10-4884-a488-6e87671984b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m6971/6971\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 4ms/step - accuracy: 0.2295 - loss: 2.7267\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.7227108478546143, 0.23081441223621368]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "model.evaluate(X_test,y_test_one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[-1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCs93aWPvD7k",
        "outputId": "5f7a85fc-cc62-4b31-9dd8-443419906843"
      },
      "id": "pCs93aWPvD7k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s3y6xr7d_2Jd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3y6xr7d_2Jd",
        "outputId": "cb724bbf-edd7-4c7c-9969-b0fac7c4fc3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# print(texts.shape)\n",
        "probs = model.predict(X_test[-1])\n",
        "logits = np.log(probs)/1\n",
        "char_id = tf.random.categorical(probs, num_samples=1)\n",
        "char_id.numpy()[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88d7c5aa-89ae-482f-89a8-f64ce70577d5",
      "metadata": {
        "id": "88d7c5aa-89ae-482f-89a8-f64ce70577d5"
      },
      "source": [
        "## Testing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a090ad16-bbd1-42aa-a359-afe60c3162a3",
      "metadata": {
        "id": "a090ad16-bbd1-42aa-a359-afe60c3162a3"
      },
      "source": [
        "This is a bit trickier than what we've done before. You need to process an input phrase, convert it to an array of ints, feed it to the model, get the logits of output, define a probability distribution,\n",
        "select an element according to that distribution, append the result to the input, and then do this over in a loop until you have generated as much output as you want. We can break this down into pieces"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "188ba04a-3717-4a0e-879e-c8e49c8faff7",
      "metadata": {
        "id": "188ba04a-3717-4a0e-879e-c8e49c8faff7"
      },
      "source": [
        "First write `next_char(text, temp)` that gets the single next character predicted using `text` as input. Remember to employ the temperature. Here's a snippet that may help\n",
        "\n",
        "```python\n",
        "  probs = # output from your model\n",
        "  logits = np.log(probs)/temp # we have to invert the softmax to get back to logits, then divide by temp\n",
        "  char_id = tf.random.categorical(probs, num_samples=1) # helper function to apply softmax and then randomly sample\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e3c720a-f149-4a07-b06f-b4c4ca55de67",
      "metadata": {
        "id": "4e3c720a-f149-4a07-b06f-b4c4ca55de67"
      },
      "outputs": [],
      "source": [
        "# your code\n",
        "\n",
        "def next_char(texts, temp):\n",
        "  texts = texts.reshape(-1,1)\n",
        "  probs = model.predict(texts, verbose=0)\n",
        "  logits = np.log(probs)/temp\n",
        "  char_id = tf.random.categorical(probs, num_samples=1)\n",
        "  return char_id.numpy()[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "190e318e-216a-4bc4-80c2-1568dbc2042e",
      "metadata": {
        "id": "190e318e-216a-4bc4-80c2-1568dbc2042e"
      },
      "source": [
        "Now write `extend_text(text, n_chars, temp)` to add any number of characters to `text` by calling `next_char` repeatedly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd8ba3c-856d-4a5f-9443-7d3ad33ac6e3",
      "metadata": {
        "id": "2cd8ba3c-856d-4a5f-9443-7d3ad33ac6e3"
      },
      "outputs": [],
      "source": [
        "# your code\n",
        "def extend_text(text, n_chars, temp):\n",
        "  texts = convert_text(text)\n",
        "  for _ in range(n_chars):\n",
        "    # print(texts)\n",
        "    newChar = next_char(texts, temp)\n",
        "    # print(newChar)\n",
        "    texts = np.append(texts, newChar)\n",
        "  final = encoder.inverse_transform(texts)\n",
        "  # for i in texts[101:]:\n",
        "  #   print(i, encoder.inverse_transform(i.reshape(1,)))\n",
        "  # print(texts)\n",
        "  return ''.join(final)\n",
        "  # return finText"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_text(text):\n",
        "  text = text.lower()\n",
        "  data = encoder.transform(list(text))\n",
        "  return data"
      ],
      "metadata": {
        "id": "fBlKONjin1Sp"
      },
      "id": "fBlKONjin1Sp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for _ in range(100):\n",
        "  if next_char(X_train[0], 1) == y_train[0]:\n",
        "    counter+=1\n",
        "\n",
        "print(counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTgETOp80lUv",
        "outputId": "ab83ab5c-f17c-431f-b225-472a3c3d3db5"
      },
      "id": "OTgETOp80lUv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(extend_text(text[:100], 20, 0.2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdfdOa8RoG-Q",
        "outputId": "a27900b4-5b61-4146-c698-56e362c74c58"
      },
      "id": "wdfdOa8RoG-Q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first citizen:\n",
            "before we proceed any further, hear me speak.\n",
            "\n",
            "all:\n",
            "speak, speak.\n",
            "\n",
            "first citizen:\n",
            "youmkwnfynchd!':&q\n",
            "3ttr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fb3f6c1-cc25-4e69-87bb-b9394b78d789",
      "metadata": {
        "id": "2fb3f6c1-cc25-4e69-87bb-b9394b78d789"
      },
      "source": [
        "Finally, generate some Shakespeare! Experiment with different seeds and seed lengths and temperatures."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2d62982-3067-48c6-b244-b2118b2f8aa1",
      "metadata": {
        "id": "e2d62982-3067-48c6-b244-b2118b2f8aa1"
      },
      "source": [
        "## Saving State"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a16d436c-e180-4de6-855b-1c6518c92523",
      "metadata": {
        "id": "a16d436c-e180-4de6-855b-1c6518c92523"
      },
      "source": [
        "When training gets this involved you really need some good practices to save your work. Here's a callback that saves progress as you train. Especially important this is on Colab, which will stop and shutdown your session if you don't make it feel special all the time.\n",
        "\n",
        "```python\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint_filepath = 'best_shakespeare_model.keras'\n",
        "\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,  # Save the entire model\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    mode='min',  # Save the model when val_loss is minimized\n",
        "    save_best_only=True  # Only save the best model\n",
        ")\n",
        "\n",
        "# Train the model with the callback\n",
        "history = model.fit(X_train, y_train, epochs=500,  validation_split=0.1, callbacks=[model_checkpoint_callback])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a7f414-1ace-40b1-b842-289d2cdea7ba",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "49a7f414-1ace-40b1-b842-289d2cdea7ba"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "# checkpoint_filepath = 'best_shakespeare_model.keras'\n",
        "\n",
        "# model_checkpoint_callback = ModelCheckpoint(\n",
        "#     filepath=checkpoint_filepath,\n",
        "#     save_weights_only=False,  # Save the entire model\n",
        "#     monitor='val_loss',  # Monitor validation loss\n",
        "#     mode='min',  # Save the model when val_loss is minimized\n",
        "#     save_best_only=True  # Only save the best model\n",
        "# )\n",
        "\n",
        "# # Train the model with the callback\n",
        "# history = model.fit(X_train, y_train_one_hot, epochs=40, batch_size=4096, validation_split=0.1, callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I6_jYiFi5XiN",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I6_jYiFi5XiN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}